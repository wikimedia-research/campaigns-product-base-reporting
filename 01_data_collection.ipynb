{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20f45415",
   "metadata": {},
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f20ec83d",
   "metadata": {},
   "source": [
    "calculate event_roll_up - \n",
    "# of events by 'active_event' and participants therein\n",
    "\n",
    "handle timing in-progress column + \n",
    "\n",
    "#Why doesn't SSAC user_id match the gu_id? - SSAC id is local\n",
    "#TODO pull account_creation date for participants and organizers FROM globaluser...if we can also pull NOT auto_created, instead of from SSAC\n",
    "\n",
    "Are we counting participants on historical test events?\n",
    "\n",
    "event_chat_url_clean - handle edge cases\n",
    "\n",
    "CHECK DATES IN QUERY VARS\n",
    "\n",
    "TODO handle organizer unregisters"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f67c36d4-b3c1-4f4d-b9fe-5f370fdce892",
   "metadata": {},
   "source": [
    "    Q = pd.to_datetime(df['app_date'], dayfirst=True).dt.to_period('Q-SEP')\n",
    "    Qstart = df['Q'].dt.asfreq('D', 's')\n",
    "    Qend = df['Q'].dt.asfreq('D', 'e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0915797b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import wmfdata as wmf\n",
    "import wmfdata as wmf\n",
    "from wmfdata import mariadb, hive, spark\n",
    "from wmfdata.utils import pct_str, pd_display_all\n",
    "\n",
    "import requests\n",
    "import re\n",
    "from urllib import request\n",
    "import json\n",
    "\n",
    "import logging\n",
    "import gc\n",
    "import weakref\n",
    "from functools import reduce\n",
    "from pathlib import Path\n",
    "import requests\n",
    "import pprint\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "#import time\n",
    "#import datetime dt\n",
    "#from datetime import datetime, timedelta, date\n",
    "#import dateutil\n",
    "\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import pandas as pd\n",
    "#%load_ext sql_magic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "be03aa58-43e7-481d-8a40-0e39f10b28d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MONTHLY -------------------------------------------\n",
    "\n",
    "#Decision log\n",
    "#https://docs.google.com/document/d/1GLMPIuDmA6_BY56SeNab66jGAEB_1YuRJ2FWSdJfYTA/edit#heading=h.ivyvml86ev1\n",
    "    \n",
    "#initial short list of wikis deployed to #https://meta.wikimedia.org/wiki/List_of_Wikipedias\n",
    "wikis = ['wikishared']#metawiki\n",
    "wikis_l = pd.DataFrame(wikis)\n",
    "wikis_l = wikis_l.rename(columns={0: 'database_code'})\n",
    "wikis_df_list = wikis_l['database_code'].tolist()\n",
    "wikis_df_tuple = tuple(wikis_df_list)\n",
    "\n",
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "METRICS_MONTH_TEXT = last_month.strftime(\"%Y-%m\")\n",
    "MEDIAWIKI_HISTORY_SNAPSHOT = last_month.strftime(\"%Y-%m\")\n",
    "metrics_month = pd.Period(METRICS_MONTH_TEXT)\n",
    "\n",
    "#fiscal_quarter = pd.PeriodIndex(metrics_month, freq='Q-JUL').strftime('Qtr%q')\n",
    "\n",
    "query_vars = dict(\n",
    "    wikis_to_query = wikis_df_tuple,\n",
    "    snapshot = MEDIAWIKI_HISTORY_SNAPSHOT,\n",
    "    metrics_month= str(metrics_month),\n",
    "    metrics_month_start= str(metrics_month.start_time), \n",
    "    metrics_month_first_day= str(metrics_month.asfreq(\"D\", how=\"start\")),\n",
    "    metrics_month_end= str((metrics_month + 1).start_time),\n",
    "    \n",
    "    year = metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y\"),\n",
    "    month = metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%m\"),\n",
    "    month_int = pd.to_numeric(metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%m\")),\n",
    "    month_int_end = pd.to_numeric((metrics_month +1).asfreq(\"D\", how=\"start\").strftime(\"%m\")),\n",
    "    \n",
    "    utc_start = metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    utc_end = (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    day_before_utc_end = ((metrics_month-1)).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    \n",
    "    metrics_month_last_day= str(metrics_month.asfreq(\"D\", how=\"end\")),\n",
    "    api_metrics_month_first_day= metrics_month.asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    api_metrics_month_next_month_first_day= (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    api_metrics_month_day_after= (metrics_month + 1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d\"),\n",
    "    \n",
    "    metrics_prev_month= str(metrics_month - 1),\n",
    "    retention_cohort= str(metrics_month - 2),\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b1713e6c-75ed-47a9-ba0b-307632aba4b3",
   "metadata": {
    "tags": []
   },
   "source": [
    "#QUARTER -------------------------------------------\n",
    "wikis = ['wikishared']#metawiki\n",
    "wikis_l = pd.DataFrame(wikis)\n",
    "wikis_l = wikis_l.rename(columns={0: 'database_code'})\n",
    "wikis_df_list = wikis_l['database_code'].tolist()\n",
    "wikis_df_tuple = tuple(wikis_df_list)\n",
    "\n",
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "METRICS_MONTH_TEXT = last_month.strftime(\"%Y-%m\")\n",
    "MEDIAWIKI_HISTORY_SNAPSHOT = last_month.strftime(\"%Y-%m\")\n",
    "metrics_month = pd.Period(METRICS_MONTH_TEXT)\n",
    "\n",
    "query_vars = dict(\n",
    "    wikis_to_query = wikis_df_tuple,\n",
    "    snapshot = MEDIAWIKI_HISTORY_SNAPSHOT,\n",
    "    #metrics_month = '2023-01',\n",
    "    metrics_month= str(metrics_month-3),\n",
    "    metrics_month_start= str((metrics_month -3).start_time), \n",
    "    metrics_month_first_day= str((metrics_month -3).asfreq(\"D\", how=\"start\")),\n",
    "    metrics_month_end= str((metrics_month).start_time),\n",
    "    \n",
    "    year = (metrics_month -3).asfreq(\"D\", how=\"start\").strftime(\"%Y\"),\n",
    "    month = (metrics_month -3).asfreq(\"D\", how=\"start\").strftime(\"%m\"),\n",
    "    month_int = pd.to_numeric((metrics_month -3).asfreq(\"D\", how=\"start\").strftime(\"%m\")),\n",
    "    month_int_end = pd.to_numeric((metrics_month -3).asfreq(\"D\", how=\"start\").strftime(\"%m\")),\n",
    "    \n",
    "    utc_start = (metrics_month -3).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    utc_end = (metrics_month).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    day_before_utc_end = ((metrics_month-1)).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    \n",
    "    metrics_month_quarter = (metrics_month).quarter \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "facab08f-094a-435d-bf33-94479fd1bf58",
   "metadata": {},
   "source": [
    "#Jan-June 2023 -------------------------------------------\n",
    "wikis = ['wikishared']#metawiki\n",
    "wikis_l = pd.DataFrame(wikis)\n",
    "wikis_l = wikis_l.rename(columns={0: 'database_code'})\n",
    "wikis_df_list = wikis_l['database_code'].tolist()\n",
    "wikis_df_tuple = tuple(wikis_df_list)\n",
    "\n",
    "last_month = datetime.date.today().replace(day=1) - datetime.timedelta(days=1)\n",
    "METRICS_MONTH_TEXT = last_month.strftime(\"%Y-%m\")\n",
    "MEDIAWIKI_HISTORY_SNAPSHOT = last_month.strftime(\"%Y-%m\")\n",
    "metrics_month = pd.Period(METRICS_MONTH_TEXT)\n",
    "\n",
    "query_vars = dict(\n",
    "    wikis_to_query = wikis_df_tuple,\n",
    "    snapshot = MEDIAWIKI_HISTORY_SNAPSHOT,\n",
    "    #metrics_month = '2023-01',\n",
    "    metrics_month= str(metrics_month-4),\n",
    "    metrics_month_start= str((metrics_month -4).start_time), \n",
    "    metrics_month_first_day= str((metrics_month -4).asfreq(\"D\", how=\"start\")),\n",
    "    metrics_month_end= str((metrics_month+1).start_time),\n",
    "    \n",
    "    year = (metrics_month -3).asfreq(\"D\", how=\"start\").strftime(\"%Y\"),\n",
    "    month = (metrics_month -4).asfreq(\"D\", how=\"start\").strftime(\"%m\"),\n",
    "    month_int = pd.to_numeric((metrics_month -4).asfreq(\"D\", how=\"start\").strftime(\"%m\")),\n",
    "    month_int_end = pd.to_numeric((metrics_month+1).asfreq(\"D\", how=\"start\").strftime(\"%m\")),\n",
    "    \n",
    "    utc_start = (metrics_month -5).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    utc_end = (metrics_month+1).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\"),\n",
    "    #day_before_utc_end = ((metrics_month-1)).asfreq(\"D\", how=\"start\").strftime(\"%Y%m%d%H%M%S\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eff1a7b3-94d0-4215-ac34-4e6ec6df2797",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'query_vars' (dict)\n"
     ]
    }
   ],
   "source": [
    "%store query_vars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e462710a",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1c0dd4c4",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-14T16.13.36_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#       campaign_events.event_status, --Status of the event (\"open\", \"closed\"...)\n",
    "#       campaign_events.event_type, -- options: generic, \n",
    "#       campaign_events.event_meeting_url, --For online events, the URL of the video call\n",
    "# ce_event_address.ceea_id,--- in person address?\n",
    "\n",
    "events_query = '''\n",
    "    SELECT \n",
    "       campaign_events.event_id, \n",
    "       campaign_events.event_name, \n",
    "       campaign_events.event_page_title,\n",
    "       campaign_events.event_page_wiki AS database_code, \n",
    "       campaign_events.event_chat_url,\n",
    "       campaign_events.event_status, \n",
    "       campaign_events.event_start_utc, \n",
    "       campaign_events.event_end_utc, \n",
    "       campaign_events.event_type, \n",
    "       campaign_events.event_meeting_type, \n",
    "       campaign_events.event_meeting_url, \n",
    "       campaign_events.event_created_at,\n",
    "       campaign_events.event_deleted_at,\n",
    "       campaign_events.event_last_edit,\n",
    "       ce_event_address.ceea_id,\n",
    "       ce_address.cea_country,\n",
    "       ce_tracking_tools.cett_tool_id, \n",
    "       ce_tracking_tools.cett_tool_event_id\n",
    "    FROM campaign_events \n",
    "    LEFT JOIN ce_event_address\n",
    "        ON campaign_events.event_id = ce_event_address.ceea_event\n",
    "    LEFT JOIN ce_address \n",
    "        ON ce_address.cea_id = ce_event_address.ceea_address\n",
    "    LEFT JOIN ce_tracking_tools\n",
    "        ON campaign_events.event_id = ce_tracking_tools.cett_event\n",
    "    WHERE \n",
    "        campaign_events.event_created_at BETWEEN \"{utc_start}\" and \"{utc_end}\" OR\n",
    "        campaign_events.event_start_utc BETWEEN \"{utc_start}\" and \"{utc_end}\" OR\n",
    "        campaign_events.event_last_edit BETWEEN \"{utc_start}\" and \"{utc_end}\"\n",
    "    \n",
    "    '''\n",
    "\n",
    "campaign_events = mariadb.run(events_query.format(**query_vars), 'wikishared')\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9c4bb2d2-330a-4f76-abca-6291785db913",
   "metadata": {},
   "source": [
    "TODO \n",
    "from sqlalchemy import create_engine\n",
    "import pandas as pd\n",
    "\n",
    "# Replace 'username', 'password', 'host', 'port', and 'database_name' with MariaDB credentials and database details\n",
    "db_uri = \"mysql+pymysql://username:password@host:port/database_name\"\n",
    "\n",
    "# Create a SQLAlchemy engine\n",
    "engine = create_engine(db_uri)\n",
    "\n",
    "# You can now use Pandas to query the database using the engine\n",
    "df = pd.read_sql_query(\"SELECT * FROM your_table\", engine)\n",
    "\n",
    "# Close the database connection when done\n",
    "engine.dispose()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc3ae42-69a9-496a-96de-0a8630a7952c",
   "metadata": {},
   "source": [
    "## Query for pageviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2b62f4-1aad-4eba-810f-f5eabb6b7967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out specific warning messages by category or message\n",
    "warnings.filterwarnings(\"ignore\", category=Warning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d56ca56c-c5f2-4005-9d21-5a666d3a4f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vars['pv_page_title'] = tuple(('Event:' + campaign_events['event_page_title'].astype(str)).unique().tolist())\n",
    "#query_vars['database_code'] = tuple(campaign_events['database_code'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2228f7ad-4266-4895-9679-36cfcac56711",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SPARK_HOME: /usr/lib/spark3\n",
      "Using Hadoop client lib jars at 3.2.0, provided by Spark.\n",
      "PYSPARK_PYTHON=/opt/conda-analytics/bin/python3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/09/12 20:35:41 WARN SparkConf: Note that spark.local.dir will be overridden by the value set by the cluster manager (via SPARK_LOCAL_DIRS in mesos/standalone/kubernetes and LOCAL_DIRS in YARN).\n",
      "23/09/12 20:35:42 WARN Utils: Service 'sparkDriver' could not bind on port 12000. Attempting port 12001.\n",
      "23/09/12 20:35:42 WARN Utils: Service 'sparkDriver' could not bind on port 12001. Attempting port 12002.\n",
      "23/09/12 20:35:42 WARN Utils: Service 'sparkDriver' could not bind on port 12002. Attempting port 12003.\n",
      "23/09/12 20:35:42 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "23/09/12 20:35:42 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n",
      "23/09/12 20:35:42 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.\n",
      "23/09/12 20:35:52 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13000. Attempting port 13001.\n",
      "23/09/12 20:35:52 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13001. Attempting port 13002.\n",
      "23/09/12 20:35:52 WARN Utils: Service 'org.apache.spark.network.netty.NettyBlockTransferService' could not bind on port 13002. Attempting port 13003.\n",
      "23/09/12 20:35:53 WARN YarnSchedulerBackend$YarnSchedulerEndpoint: Attempted to request executors before the AM has registered!\n",
      "23/09/12 20:35:58 WARN SessionState: METASTORE_FILTER_HOOK will be ignored, since hive.security.authorization.manager is set to instance of HiveAuthorizerFactory.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#TODO Consider: do we want pageviews during the contest period only?\n",
    "# replace project and database code so that it works with more than one wiki\n",
    "# domain_name.str.rstrip('(.org)') \n",
    "\n",
    "event_pageviews = '''\n",
    "SELECT  \n",
    "   sum(view_count) AS pageviews, \n",
    "   page_title AS pv_page_title\n",
    "FROM wmf.pageview_hourly  \n",
    "WHERE \n",
    "  year       = {year}            AND\n",
    "  month      >= {month_int}      AND\n",
    "  month      < {month_int_end}   AND\n",
    "  agent_type = 'user'            AND\n",
    "  project = 'meta.wikimedia'     AND\n",
    "  page_title IN {pv_page_title}\n",
    "  --page_id  = \t12164639\n",
    "GROUP BY page_title\n",
    "    '''\n",
    "\n",
    "campaign_pv = spark.run(event_pageviews.format(**query_vars))\n",
    "#remove prefix\n",
    "campaign_pv['page_title'] = campaign_pv['pv_page_title'].map(lambda x: x.lstrip('Event:'));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa5c3b7",
   "metadata": {},
   "source": [
    "### Address test rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "41c19add",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove rows called test, myFirstEvent, myfirstevent, testevent -- in any language\n",
    "string_test_remove = pd.read_csv('test_in_languages.csv')\n",
    "remove = string_test_remove['test'].unique().tolist()\n",
    "additional_strings = ['My_fun_event','The_Lion_King!','winhisintlpol_test','example', 'testezzefzefzef']\n",
    "\n",
    "remove_substrings_list = remove + additional_strings"
   ]
  },
  {
   "cell_type": "raw",
   "id": "da704322",
   "metadata": {},
   "source": [
    "#remove test entries\n",
    "ce = campaign_events[~campaign_events.event_name.str.lower().str.contains('|'.join(remove_substrings_list), na=False, case=False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "88310ca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#categorize instead of removing\n",
    "for name in remove_substrings_list:\n",
    "     campaign_events.loc[campaign_events.event_name.str.contains(name), 'event_status'] = 5 # 5 = 'test'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "145bf759",
   "metadata": {},
   "source": [
    "# Get participants count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdc9f0d0",
   "metadata": {},
   "source": [
    "# TODO potentially redo this query away from for loop as with the preceeding ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1734b65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_participants_count = []\n",
    "\n",
    "def get_participants_count_mariadb(df):\n",
    "    participants_query = '''\n",
    "    SELECT \n",
    "       campaign_events.event_id,\n",
    "       COUNT(ce_participants.cep_user_id) AS participants_register_count,\n",
    "       SUM(CASE WHEN cep_unregistered_at IS NULL THEN 0  ELSE 1 END) AS participant_unregister_count\n",
    "    FROM ce_participants\n",
    "    JOIN campaign_events\n",
    "    ON cep_event_id = campaign_events.event_id\n",
    "    WHERE campaign_events.event_id = {event_id}\n",
    "    '''\n",
    "    for event_id in df['event_id'].unique():\n",
    "        participants_r = mariadb.run(participants_query.format(event_id=event_id), 'wikishared')\n",
    "        ce_participants_count.append(participants_r)   \n",
    "    \n",
    "    return(ce_participants_count)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#campaign_events_participants = mariadb.run(participants, 'wikishared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "43b0293c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#run function\n",
    "participants = get_participants_count_mariadb(campaign_events)\n",
    "\n",
    "#object list to df, selecting only one item '_item_cache'\n",
    "participants_to_df = pd.DataFrame([vars(d) for d in participants])[['_item_cache']].apply(pd.Series)\n",
    "\n",
    "#explode dictionary to columns\n",
    "p_df = participants_to_df['_item_cache'].apply(pd.Series)\n",
    "\n",
    "\n",
    "#clean up object columns so they have only the integer values\n",
    "p_df['event_id'] = p_df['event_id'].astype(str).replace(r'\\nName: event_id, dtype: int64','', regex=True).replace('0    ','', regex=True).astype(int)\n",
    "p_df['participants_register_count'] = p_df['participants_register_count'].astype(str).replace(r'\\nName: participants_register_count, dtype: int64','', regex=True).replace('0    ','', regex=True).astype(int)\n",
    "p_df['participant_unregister_count'] = p_df['participant_unregister_count'].astype(str).replace(r'None\\nName: participant_unregister_count, dtype: object','', regex=True).replace('0    ','', regex=True).replace(r'\\nName: participant_unregister_count, dtype: float64',0, regex=True).astype(str).replace('0    ','', regex=True).apply(pd.to_numeric, errors='coerce').fillna(0.0)\n",
    "p_df['participants'] = p_df['participants_register_count'] - p_df['participant_unregister_count']\n",
    "\n",
    "#0.0\\nName: participant_total_unregisters, dtype: float64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc539a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_organizers_count = []\n",
    "\n",
    "def get_organizers_count_mariadb(df):\n",
    "    organizers_query = '''\n",
    "    SELECT \n",
    "       campaign_events.event_id,\n",
    "       COUNT(ce_organizers.ceo_user_id) AS organizers_count\n",
    "    FROM ce_organizers\n",
    "    JOIN campaign_events\n",
    "    ON ceo_event_id = campaign_events.event_id\n",
    "    WHERE campaign_events.event_id = {event_id}\n",
    "    '''\n",
    "    for event_id in df['event_id'].unique():\n",
    "        organizers_r = mariadb.run(organizers_query.format(event_id=event_id), 'wikishared')\n",
    "        ce_organizers_count.append(organizers_r)   \n",
    "    \n",
    "    return(ce_organizers_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5e61275",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n",
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#run function\n",
    "organizers = get_organizers_count_mariadb(campaign_events)\n",
    "\n",
    "#object list to df, selecting only one item '_item_cache'\n",
    "organizers_to_df = pd.DataFrame([vars(d) for d in organizers])[['_item_cache']].apply(pd.Series)\n",
    "\n",
    "#explode dictionary to columns\n",
    "o_df = organizers_to_df['_item_cache'].apply(pd.Series)\n",
    "#clean up object columns so they have only the integer values\n",
    "o_df['event_id'] = o_df['event_id'].astype(str).replace(r'\\nName: event_id, dtype: int64','', regex=True).replace('0    ','', regex=True).astype(int)\n",
    "o_df['organizers'] =o_df['organizers_count'].astype(str).replace(r'\\nName: organizers_count, dtype: int64','', regex=True).replace('0    ','', regex=True).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3c55dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_data = campaign_events.merge(p_df, how='left', on='event_id').merge(o_df, how='left', on='event_id').merge(campaign_pv, how='left', left_on='event_page_title',right_on='page_title');\n",
    "                                                                                                                "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c1f51ac",
   "metadata": {},
   "source": [
    "### clean event chat url "
   ]
  },
  {
   "cell_type": "raw",
   "id": "1cd8402d",
   "metadata": {},
   "source": [
    "dict_a = {'https://wikimedia': 'wikimedia', 'https://t.me': 'Telegram', 'https://chat.whatsapp.com': 'WhatsApp'}\n",
    "data['event_chat_url_clean'] = data['event_chat_url'].map(dict_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4bcd1f73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "event_data['event_chat_url_clean'] = event_data['event_chat_url'].replace({'https://chat.whatsapp.com': 'whatsapp',\n",
    "                                                                           'https://wikimedia.cm/': 'wikimedia',\n",
    "                                                                           'https://testgroupchaturl.org':'test',\n",
    "                                                                           'https://wikimedia.cm/':'wikimedia.cm',\n",
    "                                                                           'https://meet.google.com':'google meet',\n",
    "                                                                           'https://t.me/': 'Telegram',\n",
    "                                                                           'https://meta.wikimedia.org/wiki/Event_talk': 'event talk page',\n",
    "                                                                           'https://docs.google.com/forms': 'google forms',\n",
    "                                                                           'https://docs.google.com': 'google docs'\n",
    "                                }, regex=True).str.split('/').str[0]#.replace(r\"([^\\/]+$)\",\"\")#replace all after slash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011c67b",
   "metadata": {},
   "source": [
    "# Query for Participant usernames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fe0d8a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get event ids\n",
    "query_vars['cep_event_id_tuple'] = tuple(event_data['event_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8943d6c9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "    participant_dig = '''\n",
    "    SELECT \n",
    "       cep_event_id  AS event_id,\n",
    "       cep_user_id AS user_id,\n",
    "       cep_registered_at,\n",
    "       cep_unregistered_at\n",
    "    FROM ce_participants\n",
    "    WHERE cep_event_id IN {cep_event_id_tuple}\n",
    "\n",
    "    '''\n",
    "\n",
    "participant_dig_data = mariadb.run(participant_dig.format(**query_vars), 'wikishared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a4c96414",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get participant ids\n",
    "query_vars['cep_user_id_tuple'] = tuple(participant_dig_data['user_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "674cd69e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#GET usernames\n",
    "#fyi, this table can also provide gu_home_db AS home_db\n",
    "user_names_participants_query =  '''\n",
    "SELECT gu_name AS username,\n",
    "gu_id AS user_id\n",
    "FROM globaluser \n",
    "WHERE globaluser.gu_id IN {cep_user_id_tuple}\n",
    "'''\n",
    "\n",
    "user_names_p = mariadb.run(user_names_participants_query.format(**query_vars), 'centralauth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fa694e",
   "metadata": {},
   "source": [
    "# Query for Organizer data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9891239c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "    organizer_dig = '''\n",
    "    SELECT \n",
    "       ceo_event_id AS event_id,\n",
    "       ceo_user_id  AS user_id\n",
    "    FROM ce_organizers\n",
    "    WHERE ceo_event_id IN {cep_event_id_tuple}\n",
    "\n",
    "    '''\n",
    "\n",
    "organizer_dig_data = mariadb.run(organizer_dig.format(**query_vars), 'wikishared')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6585000a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get organizer ids\n",
    "query_vars['ceo_user_id_tuple'] = tuple(organizer_dig_data['user_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53945681",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/iflorez/.conda/envs/2023-09-11T19.34.12_iflorez/lib/python3.10/site-packages/pandas/io/sql.py:761: UserWarning: pandas only support SQLAlchemy connectable(engine/connection) ordatabase string URI or sqlite3 DBAPI2 connectionother DBAPI2 objects are not tested, please consider using SQLAlchemy\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#GET usernames\n",
    "#gu_home_db AS home_db,\n",
    "#gu_registration\n",
    "\n",
    "user_names_organizer_query =  '''\n",
    "SELECT gu_name AS username,\n",
    "gu_id AS user_id\n",
    "FROM globaluser \n",
    "WHERE globaluser.gu_id IN {ceo_user_id_tuple}\n",
    "'''\n",
    "\n",
    "user_names_organizers = mariadb.run(user_names_organizer_query.format(**query_vars), 'centralauth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913ef72c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8ba6f29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "participant_df = participant_dig_data.merge(user_names_p, how='left', on='user_id')\n",
    "participant_df['editor_type'] = 'event_participant'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1dcef2aa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "organizer_df = organizer_dig_data.merge(user_names_organizers, how='left', on='user_id')\n",
    "organizer_df['editor_type'] = 'event_organizer'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b61a55db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "editors = pd.concat([participant_df, organizer_df])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f20390dc",
   "metadata": {
    "tags": []
   },
   "source": [
    "Note user_id up to now is matched between globaluser table and campaigns table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d7e207",
   "metadata": {},
   "source": [
    "# Query for editor data"
   ]
  },
  {
   "cell_type": "raw",
   "id": "e04f0e70",
   "metadata": {
    "tags": []
   },
   "source": [
    "#https://github.com/wikimedia-research/2021-newcomer-pilot/blob/main/T286002-baseline-constructive-activation.ipynb\n",
    "\n",
    "#https://meta.wikimedia.org/wiki/Schema:ServerSideAccountCreation\n",
    "#https://meta.wikimedia.org/wiki/Schema:ServerSideEventMeta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1d4efd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get \n",
    "query_vars['usernames'] = tuple(editors['username'].unique().tolist())\n",
    "query_vars['user_ids'] = tuple(editors['user_id'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b759fd35",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#New Campaign User - https://phabricator.wikimedia.org/T329382\n",
    "#a) this is the first logging of their username (a global account)\n",
    "#b) it is not auto logged\n",
    "#c) i) returnTo field = campaign page and the campaign is happening or going to happen in the next 30 days \n",
    "#OR\n",
    "#c) ii) account created within 30 days of their signing up for an event \n",
    "\n",
    "#LPAD auto coerces number to string on Spark\n",
    "#event.SSAC has 90 days of data, but event_sanitized.SSAC will have longer time data for columns in allowlist. \n",
    "# FROM event.serversideaccountcreation\n",
    "#--event.returnTo AS acp,\n",
    "\n",
    "\n",
    "#This gives rows that are between x and y months prior. \n",
    "acp_ssac = spark.run(\"\"\"\n",
    "SELECT event.userName AS username,\n",
    "    CONCAT(cast(year as string), '-', LPAD(cast(month as string), 2, '0'), '-', LPAD(day, 2, '0')) AS `ac_date`\n",
    "FROM event_sanitized.serversideaccountcreation  \n",
    "WHERE \n",
    "    event.userName IN {usernames} AND\n",
    "    event.isselfmade = true AND\n",
    "    CONCAT(cast(year as string), '-', LPAD(cast(month as string), 2, '0'), '-', LPAD(day, 2, '0')) >= date_sub(current_date(), 90)\n",
    "\"\"\".format(**query_vars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2f725c21-625c-40f3-b0da-bc89c78773c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO this data could also be pulled above for participants and organizers FROM globaluser...if we can also pull NOT auto_created\n",
    "acp_ssac['ac_date'] = pd.to_datetime(acp_ssac['ac_date'], format = \"%Y-%m-%d\")#.dt.strftime(\"%Y%m%d\")\n",
    "editors['cep_registered_at'] = pd.to_datetime(editors['cep_registered_at'], format = \"%Y%m%d%H%M%S\")#.dt.strftime(\"%Y%m%d\")\n",
    "\n",
    "\n",
    "potentially_new_people = (acp_ssac.merge(editors[['user_id', 'event_id', 'cep_registered_at','username']], how='left', on='username')\n",
    "                                 .merge(event_data[['event_id', 'event_created_at', 'event_start_utc','event_end_utc']], how='left', on='event_id')\n",
    "                         )\n",
    "\n",
    "#time delta (days) between account_creation date and 'cep_registered_at'\n",
    "potentially_new_people['ac_reg_delta']=(potentially_new_people.cep_registered_at-potentially_new_people.ac_date).dt.days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d2ddec56-9a4b-4ee4-a69e-ca002fb06123",
   "metadata": {},
   "outputs": [],
   "source": [
    "#keep only rows of new folks that joined an event within 30 days of registering\n",
    "new_people = potentially_new_people[potentially_new_people['ac_reg_delta']<=30]\n",
    "\n",
    "#keep only their first registration (sort by name and sort by their registration timestamps)\n",
    "new_people_selects_prep = new_people[['username','ac_date','ac_reg_delta','cep_registered_at','event_created_at','event_id']]\n",
    "new_people_selects_prep.sort_values(by=[\"username\",'cep_registered_at']).drop_duplicates('username')\n",
    "\n",
    "#Remove duplicates by columns username, keeping the row with the lowest value in column ac_reg_delta\n",
    "new_people_selects = new_people_selects_prep.groupby('username', group_keys=False).apply(lambda x: x.loc[x.ac_reg_delta.idxmin()])\n",
    "new_people_selects.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0d0bb03e-8c2e-43c4-ae17-04e5a54223c7",
   "metadata": {},
   "source": [
    "new_people_selects = new_people_selects.groupby('username', group_keys=False).apply(lambda x: x.loc[x.ac_reg_delta.idxmin()])\n",
    "new_people_by_event = new_people_selects.groupby('event_id')['username'].nunique().reset_index().rename(columns={'username':'new_users_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "870bcfb6-17ab-4331-8ab6-d1cabb3c6b2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_people_selects['username'].duplicated().any()\n",
    "\n",
    "#check for duplicates that were missed\n",
    "#pd.concat(g for _, g in new_people_selects.groupby(\"username\") if len(g) > 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "e2f6bcc9-ef1e-4c82-b4f2-5550161abd7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a new_df with the above showing only the count of new people per each event\n",
    "new_people_by_event = new_people_selects.groupby('event_id')['username'].nunique().reset_index().rename(columns={'username':'new_users_count'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "609e755b-7d2a-40d3-a6f3-1093504fcfbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#COUNT(DISTINCT em.user_id) AS total_accounts,\n",
    "#MIN(em.user_registration) AS em_registration,\n",
    "    \n",
    "#SUM(em.edits) AS total_edits, \n",
    "#SUM(em.content_edits) AS total_content_edits, \n",
    "#SUM(em.visual_edits) AS total_visual_edits, \n",
    "#SUM(CAST(em.mobile_app_edits AS INT))+SUM(CAST(em.mobile_web_edits AS INT)) AS mobile_edits,\n",
    "\n",
    "#MAX(event_timestamp) AS last_activity_date,\n",
    "#MIN(event_user_first_edit_timestamp) AS first_activity_date,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "136bbf30-7a59-487d-a2ac-e3590f44d0b5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#https://wikitech.wikimedia.org/wiki/Analytics/Data_Lake/Edits/MediaWiki_history\n",
    "#https://github.com/IreneFlorez/GLOW/blob/master/scripts/data_wrangling/8a_collect_contest_editors_region.ipynb\n",
    "#https://github.com/wikimedia/analytics-refinery/blob/master/oozie/edit/hourly/edit_hourly.hql -- for user_tenure_bucket and user_edit_count_buckets\n",
    "#https://gerrit.wikimedia.org/r/c/analytics/refinery/+/889766/2/hql/edit/edit_hourly.hql --latest update\n",
    "#       --MAX(event_timestamp) AS last_activity_date,\n",
    "editor_data_query_results_em = spark.run(\"\"\"\n",
    "\n",
    "WITH editor_month AS (\n",
    "SELECT em.user_name AS username,\n",
    "    COUNT(DISTINCT em.wiki) AS total_wikis, \n",
    "    SUM(CAST(em.bot_by_group AS INT)) AS bot_history,\n",
    "    COUNT(DISTINCT CASE WHEN em.wiki LIKE 'commonswiki' THEN  em.wiki END) AS commons_editor,\n",
    "    COUNT(DISTINCT CASE WHEN em.wiki LIKE 'wikidatawiki' THEN  em.wiki END) AS wikidata_editor,\n",
    "    CASE\n",
    "        WHEN SUM(em.edits) < 5 THEN '1-4'\n",
    "        WHEN SUM(em.edits) >= 5 AND SUM(em.edits) < 100 THEN '5-99'\n",
    "        WHEN SUM(em.edits) >= 100 AND SUM(em.edits) < 1000 THEN '100-999'\n",
    "        WHEN SUM(em.edits) >= 1000 AND SUM(em.edits) < 10000 THEN '1000-9999'\n",
    "        WHEN SUM(em.edits) >= 10000 THEN '10000+'\n",
    "        ELSE 'Undefined'\n",
    "    END AS user_edit_count_bucket\n",
    "from wmf_product.editor_month em\n",
    "WHERE user_name IN {usernames} \n",
    "GROUP BY user_name\n",
    "),\n",
    "\n",
    "mwh AS (\n",
    "SELECT DISTINCT(event_user_text)  AS username,\n",
    "       MAX(event_timestamp) OVER (PARTITION BY event_user_text)  AS last_activity_date,\n",
    "       --MIN(event_user_first_edit_timestamp) AS first_activity_date,\n",
    "       MAX(UNIX_TIMESTAMP(event_timestamp, 'yyyy-MM-dd HH:mm:ss.SSS') - UNIX_TIMESTAMP(\n",
    "            LEAST(\n",
    "                COALESCE(event_user_registration_timestamp, event_user_creation_timestamp, event_user_first_edit_timestamp),\n",
    "                COALESCE(event_user_creation_timestamp, event_user_first_edit_timestamp, event_user_registration_timestamp),\n",
    "                COALESCE(event_user_first_edit_timestamp, event_user_registration_timestamp, event_user_creation_timestamp)\n",
    "            ),'yyyy-MM-dd HH:mm:ss.SSS')) OVER (PARTITION BY event_user_text)  AS user_tenure\n",
    "FROM wmf.mediawiki_history \n",
    "WHERE  \n",
    "    event_entity = 'revision'          AND\n",
    "    event_type = 'create'              AND\n",
    "    event_user_text IN {usernames}    AND\n",
    "    snapshot == '{snapshot}'        AND\n",
    "    event_timestamp IS NOT NULL        \n",
    ")\n",
    "\n",
    "\n",
    "SELECT\n",
    "    mwh.username, \n",
    "    last_activity_date,\n",
    "    CASE\n",
    "        WHEN user_tenure < 86400 THEN 'Under 1 day'\n",
    "        WHEN user_tenure >= 86400 AND user_tenure < 7*86400 THEN '1 to 7 days'\n",
    "        WHEN user_tenure >= 7*86400 AND user_tenure < 30*86400 THEN '7 to 30 days'\n",
    "        WHEN user_tenure >= 30*86400 AND user_tenure < 90*86400 THEN '30 to 90 days'\n",
    "        WHEN user_tenure >= 90*86400 AND user_tenure < 365*86400 THEN '90 days to 1 year'\n",
    "        WHEN user_tenure >= 365*86400 AND user_tenure < 1095*86400 THEN '1 to 3 years'\n",
    "        WHEN user_tenure >= 1095*86400 AND user_tenure < 3650*86400 THEN '3 to 10 years'\n",
    "        WHEN user_tenure >= 3650*86400 THEN 'Over 10 years'\n",
    "        ELSE 'Undefined'\n",
    "    END AS user_tenure_bucket, --Bucketed time between user creation and edit (Under 1 day, 1 to 7 days, 7 to 30 days, ..., Over 10 years, Undefined)\n",
    "    total_wikis,\n",
    "    bot_history,\n",
    "    commons_editor,\n",
    "    wikidata_editor\n",
    "FROM editor_month\n",
    "LEFT JOIN mwh \n",
    "ON editor_month.username = mwh.username\n",
    "\n",
    "\"\"\".format(**query_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7544e896-9aa6-4360-9721-e9826a78387e",
   "metadata": {},
   "source": [
    "# Add in countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "88754436-3ae9-4f59-8de6-55e1f36ea764",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_vars['countries'] = tuple(event_data['cea_country'].dropna().unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1b6c9970-3738-4ab1-8234-5bea47f2c534",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "countries_df = spark.run(\"\"\"\n",
    "SELECT\n",
    "    r.wmf_region,\n",
    "    r.canonical_country_name AS cea_country\n",
    "FROM gdi.country_meta_data AS r\n",
    "WHERE \n",
    "    r.canonical_country_name IN {countries}\n",
    "\"\"\".format(**query_vars))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cab44505",
   "metadata": {},
   "source": [
    "# Outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d95efd",
   "metadata": {},
   "source": [
    "## Event"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ec0e2c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "organizers_only_from_editor_data = editors[editors['editor_type']=='event_organizer'][['event_id','username']]\n",
    "organizers_only_from_editor_data = organizers_only_from_editor_data.rename(columns={'username':'organizer_username'}) \n",
    "\n",
    "#count of organizers per event\n",
    "#organizers_per_event = organizers_only_from_editor_data.groupby('event_id')['organizer_username'].nunique().reset_index().rename(columns={'organizer_username':'organizer_count'})\n",
    "\n",
    "#unique orgnizers per event to a list\n",
    "orgnizers_list_per_event = organizers_only_from_editor_data.groupby('event_id')['organizer_username'].unique().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8b236f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "event_roster = (event_data.merge(orgnizers_list_per_event, how='left', on='event_id')\n",
    "                          .merge(countries_df, how='left', on='cea_country')\n",
    "               )\n",
    "\n",
    "#merge(organizers_per_event, how='left', on='event_id')\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "28180467-389d-4af7-8c7d-bbc6f83c6b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_selection = ['participants_register_count', 'participant_unregister_count','participants','event_id', 'event_name', 'event_page_title', 'database_code','event_chat_url','event_status', 'event_start_utc', 'event_end_utc', 'event_type', 'event_meeting_type','event_meeting_url','event_created_at', 'event_deleted_at', 'event_last_edit','ceea_id', 'cea_country','cett_tool_id', 'cett_tool_event_id','pageviews', 'pv_page_title', 'page_title', 'event_chat_url_clean', 'wmf_region', 'new_users_count']\n",
    "\n",
    "#list that can't be used in drop_duplicates:\n",
    "#'organizers_count', 'organizer_username\n",
    "                                \n",
    "base_view = (event_roster.merge(new_people_by_event, how='left', on='event_id')\n",
    "             .drop_duplicates(subset=subset_selection)\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d4495c7e-58bc-48c7-b866-d8ee2b52c6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reorder columns\n",
    "base_view = base_view[['event_id',\n",
    "                       'event_page_title',\n",
    "                       'cea_country','wmf_region', \n",
    "                       'pageviews',\n",
    "                       'organizers','organizer_username',\n",
    "                       'new_users_count',\n",
    "                       'participants',\n",
    "                       'event_status',\n",
    "                       'event_created_at','event_start_utc', 'event_end_utc', 'event_last_edit','event_deleted_at',\n",
    "                       'event_chat_url_clean','cett_tool_id','cett_tool_event_id','event_meeting_url',\n",
    "                       'participants_register_count','participant_unregister_count',\n",
    "                       'event_type','event_meeting_type'\n",
    "                      ]].rename(columns={'cea_country': 'country',\n",
    "                                        #'participant_total_registers':'participants'\n",
    "                                        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "009141be",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_view.to_csv('output/wrangling/base_view.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0de4fa1-c999-46fd-bd4e-9106aa54ec96",
   "metadata": {},
   "source": [
    "## Editors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "60719cee-7e09-46ad-a916-3d1334456057",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#merge \n",
    "editor_data = (editors.merge(editor_data_query_results_em, \n",
    "                             how='left', \n",
    "                             on='username'\n",
    "                            )\n",
    "                      .merge(acp_ssac, \n",
    "                             how='left', \n",
    "                             on='username'\n",
    "                            )\n",
    "                      .merge(base_view[['event_id',\n",
    "                                        'country',\n",
    "                                        'wmf_region',\n",
    "                                        'event_status'\n",
    "                                       ]],\n",
    "                             how='left', \n",
    "                             on=['event_id']\n",
    "                            )\n",
    "                      .merge(new_people_selects[['username', 'ac_reg_delta'\n",
    "                                                ]],\n",
    "                             how='left', \n",
    "                             on='username'\n",
    "                            )\n",
    "                      \n",
    "              )               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "653c4aee-2ffd-43d2-a7bd-817ea6fc5330",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor_data.to_csv('output/wrangling/editors.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a394342-5e92-439a-9fe6-a7e67453a36d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
